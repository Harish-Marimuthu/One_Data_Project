# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DAhTme3p786N2mWxvliwRqixh5lq3W6u
"""

# app.py

import os
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from transformers import pipeline
from langchain.docstore.document import Document

# === Helper Functions ===

@st.cache_data(show_spinner=False)
def load_pdf(file):
    reader = PdfReader(file)
    text = ""
    for page in reader.pages:
        content = page.extract_text()
        if content:
            text += content
    return text

def create_documents_with_source(text, source_name):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = splitter.split_text(text)
    return [Document(page_content=chunk, metadata={"source": source_name}) for chunk in chunks]

# @st.cache_resource(show_spinner=False)
def create_vectorstore_from_docs(_documents):
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectorstore = FAISS.from_documents(_documents, embeddings)
    return vectorstore


# @st.cache_resource(show_spinner=False)
def create_qa_chain(vectorstore):
    pipe = pipeline("text2text-generation", model="google/flan-t5-base", max_length=512)
    llm = HuggingFacePipeline(pipeline=pipe)
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 2})
    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")
    return qa_chain

# === Streamlit UI ===

st.title("ðŸ“š Personal Research Assistant Agent")
st.write("Upload two PDFs, ask your question, and get answers from the content.")

# Upload two PDF files
pdf1 = st.file_uploader("Upload PDF 1", type="pdf")
pdf2 = st.file_uploader("Upload PDF 2", type="pdf")

if pdf1 and pdf2:
    with st.spinner("Reading and processing PDFs..."):
        text1 = load_pdf(pdf1)
        text2 = load_pdf(pdf2)
        docs1 = create_documents_with_source(text1, "PDF_1")
        docs2 = create_documents_with_source(text2, "PDF_2")
        all_docs = docs1 + docs2
        vectorstore = create_vectorstore_from_docs(all_docs)
        qa_chain = create_qa_chain(vectorstore)

    question = st.text_input("Enter your research question:")

    if question:
        with st.spinner("Generating answer..."):
            result = qa_chain.invoke({"query": question})
            st.success("Answer:")
            st.write(result["result"])